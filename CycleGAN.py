import numpy as np
import tensorflow as tf
from DataLoader import DataLoader
from generator import Generator
from discriminator import Discriminator


class CycleGAN():
    def __init__(self, opt, is_training=True):
        self.opt = opt
        self.is_training = is_training

        # create an iterator for our datasets
        self.dataloader = DataLoader(self.opt)

        # build the Generator models for each GAN
        self.G = Generator(self.opt, self.is_training, name='G')
        self.F = Generator(self.opt, self.is_training, name='F')

        # build the Discriminator models for each GAN only if in training phase
        if is_training:
            self.D_X = Discriminator(self.opt, self.is_training, name='D_X')
            self.D_Y = Discriminator(self.opt, self.is_training, name='D_Y')

        # create placeholders for real and generated training/test data
        self.realA = tf.placeholder(tf.float32, shape=(None, self.opt.image_size, self.opt.image_size, self.in_channels) 'Real Set A')
        self.realB = tf.placeholder(tf.float32, shape=(None, self.opt.image_size, self.opt.image_size, self.out_channels) 'Real Set B')
        self.fakeA = tf.placeholder(tf.float32, shape=(None, self.opt.image_size, self.opt.image_size, self.in_channels) 'Fake Set A')
        self.fakeB = tf.placeholder(tf.float32, shape=(None, self.opt.image_size, self.opt.image_size, self.out_channels) 'Fake Set B')

    def set_input(self):
        pass

    def build_model(self):
        pass

    def G_loss(self, D, fake, real_label=1.0, epsilon=1e-12):
        """
            Find the generator loss using either least squared error or negative log likelihood.

            Args:
                D: Discriminator model
                fake: Fake images generated by the Generator model
                real_label: The value assigned to a real label (fake label would be 0.0)
                epsilon: Small value to ensure log of zero is not taken

            Returns:
                generator_loss: The loss of the Generator model
        """
        if self.opt.gan_mode is 'lsgan': # least squared error
            generator_loss = tf.reduce_mean(tf.squared_difference(D(fake), real_label))
        elif self.opt.gan_mode is 'vanilla': # negative log likelihood
            generator_loss = -tf.reduce_mean(tf.log(D(fake) + epsilon))

        return generator_loss

    def D_loss(self, D, real, fake, real_label=1.0, epsilon=1e-12):
        """
            Find the discriminator loss using either least squared error or negative log likelihood.

            Args:
                D: Discriminator model
                real: Real images from the dataset
                fake: Fake images generated by the Generator model
                real_label: The value assigned to a real label (fake label would be 0.0)
                epsilon: Small value to ensure log of zero is not taken

            Returns:
                discriminator_loss: The loss of the Discriminator model
        """
        if self.opt.gan_mode is 'lsgan': # least squared error
            discriminator_loss = tf.reduce_mean(tf.squared_difference(D(real), real_label))
                                 + tf.reduce_mean(tf.square(D(fake)))
        elif self.opt.gan_mode is 'vanilla': # negative log likelihood
            discriminator_loss = -1 * (tf.reduce_mean(tf.log(D(real) + epsilon))
                                       + tf.reduce_mean(tf.log((1 - D(fake)) + epsilon)))

        return discriminator_loss

    def cycle_consistency_loss(self, G, F, x, y):
        """
            Find the forward loss and backward loss of our GANS.

            Use L1 norm for calculating error between mapping and real.

            Args:
                G: Generator that maps Domain X -> Domain Y
                F: Generator that maps Domain Y -> Domain X
                x: Real images from Domain X
                y: Real images from Domain Y

            Returns:
                loss: The cycle consistency loss of our network
        """
        forward_loss = tf.reduce_mean(tf.abs(F(G(x) - x)))
        backward_loss = tf.reduce_mean(tf.abs(G(F(y) - y)))
        loss = (self.opt.lambda_A * forward_loss) + (self.opt.lambda_B * backward_loss)
        return loss
